{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"data":{"text/plain":["'\\nAuthor: Ioannis Kakogeorgiou\\nEmail: gkakogeorgiou@gmail.com\\nPython Version: 3.7.10\\nDescription: train.py includes the training process for the\\n             pixel-level semantic segmentation.\\n'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["# -*- coding: utf-8 -*-\n","'''\n","Author: Ioannis Kakogeorgiou\n","Email: gkakogeorgiou@gmail.com\n","Python Version: 3.7.10\n","Description: train.py includes the training process for the\n","             pixel-level semantic segmentation.\n","'''"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import os\n","import ast\n","import sys\n","import json\n","import random\n","import logging\n","import argparse\n","import numpy as np\n","from tqdm import tqdm\n","from os.path import dirname as up"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/hgumnur/miniconda3/envs/marida/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","import torchvision.transforms as transforms\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["sys.path.append(up(os.path.abspath('')))\n","from unet import AttentionUNet\n","from dataloader import GenDEBRIS, bands_mean, bands_std, RandomRotationTransform , class_distr, gen_weights"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["sys.path.append(os.path.join(up(up(os.path.abspath(''))), 'utils'))\n","from metrics import Evaluation"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["root_path = up(up(os.path.abspath('')))"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["logging.basicConfig(filename=os.path.join(root_path, 'logs','log_unet.log'), filemode='a',level=logging.INFO, format='%(name)s - %(levelname)s - %(message)s')\n","logging.info('*'*10)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def seed_all(seed):\n","    # Pytorch Reproducibility\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.cuda.manual_seed(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    \n","def seed_worker(worker_id):\n","    # DataLoader Workers Reproducibility\n","    worker_seed = torch.initial_seed() % 2**32\n","    np.random.seed(worker_seed)\n","    random.seed(worker_seed)"]},{"cell_type":"markdown","metadata":{},"source":["#############################################################<br>\n","Training                                                    #<br>\n","#############################################################"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["def main(options):\n","    # Reproducibility\n","    # Limit the number of sources of nondeterministic behavior \n","    seed_all(0)\n","    g = torch.Generator()\n","    g.manual_seed(0)\n","    \n","    # Tensorboard\n","    writer = SummaryWriter(os.path.join(root_path, 'logs', options['tensorboard']))\n","    \n","    # Transformations\n","    \n","    transform_train = transforms.Compose([transforms.ToTensor(),\n","                                    RandomRotationTransform([-90, 0, 90, 180]),\n","                                    transforms.RandomHorizontalFlip()])\n","    \n","    transform_test = transforms.Compose([transforms.ToTensor()])\n","    \n","    standardization = transforms.Normalize(bands_mean, bands_std)\n","    \n","    # Construct Data loader\n","    \n","    if options['mode']=='train':\n","        \n","        dataset_train = GenDEBRIS('train', transform=transform_train, standardization = standardization, agg_to_water = options['agg_to_water'])\n","        dataset_test = GenDEBRIS('val', transform=transform_test, standardization = standardization, agg_to_water = options['agg_to_water'])\n","        \n","        train_loader = DataLoader(  dataset_train, \n","                                    batch_size = options['batch'], \n","                                    shuffle = True,\n","                                    num_workers = options['num_workers'],\n","                                    pin_memory = options['pin_memory'],\n","                                    prefetch_factor = options['prefetch_factor'],\n","                                    persistent_workers= options['persistent_workers'],\n","                                    worker_init_fn=seed_worker,\n","                                    generator=g)\n","        \n","        test_loader = DataLoader(   dataset_test, \n","                                    batch_size = options['batch'], \n","                                    shuffle = False,\n","                                    num_workers = options['num_workers'],\n","                                    pin_memory = options['pin_memory'],\n","                                    prefetch_factor = options['prefetch_factor'],\n","                                    persistent_workers= options['persistent_workers'],\n","                                    worker_init_fn=seed_worker,\n","                                    generator=g)\n","        \n","    elif options['mode']=='test':\n","        \n","        dataset_test = GenDEBRIS('test', transform=transform_test, standardization = standardization, agg_to_water = options['agg_to_water'])\n","    \n","        test_loader = DataLoader(   dataset_test, \n","                                    batch_size = options['batch'], \n","                                    shuffle = False,\n","                                    num_workers = options['num_workers'],\n","                                    pin_memory = options['pin_memory'],\n","                                    prefetch_factor = options['prefetch_factor'],\n","                                    persistent_workers= options['persistent_workers'],\n","                                    worker_init_fn=seed_worker,\n","                                    generator=g)\n","    else:\n","        raise                        \n","    \n","    # Use gpu or cpu\n","    \n","    if torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","    else:\n","        device = torch.device(\"cpu\")\n","        \n","    model = AttentionUNet(options['input_channels'], \n","                options['output_channels'], \n","                options['hidden_channels'])\n","    model.to(device)\n","\n","    # Load model from specific epoch to continue the training or start the evaluation\n","    if options['resume_from_epoch'] > 1:\n","        \n","        resume_model_dir = os.path.join(options['checkpoint_path'], str(options['resume_from_epoch']))\n","        model_file = os.path.join(resume_model_dir, 'model.pth')\n","        logging.info('Loading model files from folder: %s' % model_file)\n","        checkpoint = torch.load(model_file, map_location = device)\n","        model.load_state_dict(checkpoint)\n","        del checkpoint  # dereference\n","        if torch.cuda.is_available():\n","            torch.cuda.empty_cache()\n","    global class_distr\n","    # Aggregate Distribution Mixed Water, Wakes, Cloud Shadows, Waves with Marine Water\n","    if options['agg_to_water']:\n","        agg_distr = sum(class_distr[-4:]) # Density of Mixed Water, Wakes, Cloud Shadows, Waves\n","        class_distr[6] += agg_distr       # To Water\n","        class_distr = class_distr[:-4]    # Drop Mixed Water, Wakes, Cloud Shadows, Waves\n","\n","    # Weighted Cross Entropy Loss & adam optimizer\n","    weight = gen_weights(class_distr, c = options['weight_param'])\n","    criterion = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction= 'mean', weight=weight.to(device))\n","    optimizer = torch.optim.Adam(model.parameters(), lr=options['lr'], weight_decay=options['decay'])\n","\n","    # Learning Rate scheduler\n","    if options['reduce_lr_on_plateau']==1:\n","        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n","    else:\n","        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, options['lr_steps'], gamma=0.1, verbose=True)\n","\n","    # Start training\n","    start = options['resume_from_epoch'] + 1\n","    epochs = options['epochs']\n","    eval_every = options['eval_every']\n","    \n","    # Write model-graph to Tensorboard\n","    if options['mode']=='train':\n","        dataiter = iter(train_loader)\n","        image_temp, _ = next(dataiter)\n","        writer.add_graph(model, image_temp.to(device))\n","        \n","        ###############################################################\n","        # Start Training                                              #\n","        ###############################################################\n","        model.train()\n","        \n","        for epoch in range(start, epochs+1):\n","            training_loss = []\n","            training_batches = 0\n","            \n","            i_board = 0\n","            for (image, target) in tqdm(train_loader, desc=\"training\"):\n","                \n","                image = image.to(device)\n","                target = target.to(device)\n","    \n","                optimizer.zero_grad()\n","                \n","                logits = model(image)\n","                \n","                loss = criterion(logits, target)\n","    \n","                loss.backward()\n","    \n","                training_batches += target.shape[0]\n","    \n","                training_loss.append((loss.data*target.shape[0]).tolist())\n","                \n","                optimizer.step()\n","                \n","                # Write running loss\n","                writer.add_scalar('training loss', loss , (epoch - 1) * len(train_loader)+i_board)\n","                i_board+=1\n","            \n","            logging.info(\"Training loss was: \" + str(sum(training_loss) / training_batches))\n","            \n","            ###############################################################\n","            # Start Evaluation                                            #\n","            ###############################################################\n","            \n","            if epoch % eval_every == 0 or epoch==1:\n","                model.eval()\n","    \n","                test_loss = []\n","                test_batches = 0\n","                y_true = []\n","                y_predicted = []\n","                \n","                with torch.no_grad():\n","                    for (image, target) in tqdm(test_loader, desc=\"testing\"):\n","    \n","                        image = image.to(device)\n","                        target = target.to(device)\n","    \n","                        logits = model(image)\n","                        \n","                        loss = criterion(logits, target)\n","                                    \n","                        # Accuracy metrics only on annotated pixels\n","                        logits = torch.movedim(logits, (0,1,2,3), (0,3,1,2))\n","                        logits = logits.reshape((-1,options['output_channels']))\n","                        target = target.reshape(-1)\n","                        mask = target != -1\n","                        logits = logits[mask]\n","                        target = target[mask]\n","                        \n","                        probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n","                        target = target.cpu().numpy()\n","                        \n","                        test_batches += target.shape[0]\n","                        test_loss.append((loss.data*target.shape[0]).tolist())\n","                        y_predicted += probs.argmax(1).tolist()\n","                        y_true += target.tolist()\n","                            \n","                        \n","                    y_predicted = np.asarray(y_predicted)\n","                    y_true = np.asarray(y_true)\n","                    \n","                    ####################################################################\n","                    # Save Scores to the .log file and visualize also with tensorboard #\n","                    ####################################################################\n","                    \n","                    acc = Evaluation(y_predicted, y_true)\n","                    logging.info(\"\\n\")\n","                    logging.info(\"Test loss was: \" + str(sum(test_loss) / test_batches))\n","                    logging.info(\"STATISTICS AFTER EPOCH \" +str(epoch) + \": \\n\")\n","                    logging.info(\"Evaluation: \" + str(acc))\n","    \n","    \n","                    logging.info(\"Saving models\")\n","                    model_dir = os.path.join(options['checkpoint_path'], str(epoch))\n","                    os.makedirs(model_dir, exist_ok=True)\n","                    torch.save(model.state_dict(), os.path.join(model_dir, 'model.pth'))\n","                    \n","                    writer.add_scalars('Loss per epoch', {'Test loss':sum(test_loss) / test_batches, \n","                                                          'Train loss':sum(training_loss) / training_batches}, \n","                                       epoch)\n","                    \n","                    writer.add_scalar('Precision/test macroPrec', acc[\"macroPrec\"] , epoch)\n","                    writer.add_scalar('Precision/test microPrec', acc[\"microPrec\"] , epoch)\n","                    writer.add_scalar('Precision/test weightPrec', acc[\"weightPrec\"] , epoch)\n","                    \n","                    writer.add_scalar('Recall/test macroRec', acc[\"macroRec\"] , epoch)\n","                    writer.add_scalar('Recall/test microRec', acc[\"microRec\"] , epoch)\n","                    writer.add_scalar('Recall/test weightRec', acc[\"weightRec\"] , epoch)\n","                    \n","                    writer.add_scalar('F1/test macroF1', acc[\"macroF1\"] , epoch)\n","                    writer.add_scalar('F1/test microF1', acc[\"microF1\"] , epoch)\n","                    writer.add_scalar('F1/test weightF1', acc[\"weightF1\"] , epoch)\n","                    \n","                    writer.add_scalar('IoU/test MacroIoU', acc[\"IoU\"] , epoch)\n","                    \n","    \n","                if options['reduce_lr_on_plateau'] == 1:\n","                    scheduler.step(sum(test_loss) / test_batches)\n","                else:\n","                    scheduler.step()\n","                    \n","                model.train()\n","               \n","    # CODE ONLY FOR EVALUATION - TESTING MODE !\n","    elif options['mode']=='test':\n","        \n","        model.eval()\n","        test_loss = []\n","        test_batches = 0\n","        y_true = []\n","        y_predicted = []\n","        \n","        with torch.no_grad():\n","            for (image, target) in tqdm(test_loader, desc=\"testing\"):\n","                image = image.to(device)\n","                target = target.to(device)\n","                logits = model(image)\n","                \n","                loss = criterion(logits, target)\n","                # Accuracy metrics only on annotated pixels\n","                logits = torch.movedim(logits, (0,1,2,3), (0,3,1,2))\n","                logits = logits.reshape((-1,options['output_channels']))\n","                target = target.reshape(-1)\n","                mask = target != -1\n","                logits = logits[mask]\n","                target = target[mask]\n","                \n","                probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n","                target = target.cpu().numpy()\n","                \n","                test_batches += target.shape[0]\n","                test_loss.append((loss.data*target.shape[0]).tolist())\n","                y_predicted += probs.argmax(1).tolist()\n","                y_true += target.tolist()\n","                \n","            y_predicted = np.asarray(y_predicted)\n","            y_true = np.asarray(y_true)\n","            \n","            ####################################################################\n","            # Save Scores to the .log file                                     #\n","            ####################################################################\n","            acc = Evaluation(y_predicted, y_true)\n","            logging.info(\"\\n\")\n","            logging.info(\"Test loss was: \" + str(sum(test_loss) / test_batches))\n","            logging.info(\"STATISTICS: \\n\")\n","            logging.info(\"Evaluation: \" + str(acc))"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-02-09 13:49:18.836879: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","Load train set to memory: 100%|██████████| 694/694 [00:53<00:00, 13.07it/s]\n","Load val set to memory:  77%|███████▋  | 253/328 [00:13<00:04, 18.45it/s]"]}],"source":["parser = argparse.ArgumentParser()\n","\n","# Options\n","parser.add_argument('--agg_to_water', default=True, type=bool,  help='Aggregate Mixed Water, Wakes, Cloud Shadows, Waves with Marine Water')\n","\n","parser.add_argument('--mode', default='train', help='select between train or test ')\n","parser.add_argument('--epochs', default=45, type=int, help='Number of epochs to run')\n","parser.add_argument('--batch', default=5, type=int, help='Batch size')\n","parser.add_argument('--resume_from_epoch', default=0, type=int, help='load model from previous epoch')\n","\n","parser.add_argument('--input_channels', default=11, type=int, help='Number of input bands')\n","parser.add_argument('--output_channels', default=11, type=int, help='Number of output classes')\n","parser.add_argument('--hidden_channels', default=16, type=int, help='Number of hidden features')\n","parser.add_argument('--weight_param', default=1.03, type=float, help='Weighting parameter for Loss Function')\n","\n","# Optimization\n","parser.add_argument('--lr', default=2e-4, type=float, help='learning rate')\n","parser.add_argument('--decay', default=0, type=float, help='learning rate decay')\n","parser.add_argument('--reduce_lr_on_plateau', default=0, type=int, help='reduce learning rate when no increase (0 or 1)')\n","parser.add_argument('--lr_steps', default='[40]', type=str, help='Specify the steps that the lr will be reduced')\n","\n","# Evaluation/Checkpointing\n","parser.add_argument('--checkpoint_path', default=os.path.join(up(os.path.abspath(os.path.abspath(''))), 'trained_models'), help='folder to save checkpoints into (empty = this folder)')\n","parser.add_argument('--eval_every', default=1, type=int, help='How frequently to run evaluation (epochs)')\n","\n","# misc\n","parser.add_argument('--num_workers', default=1, type=int, help='How many cpus for loading data (0 is the main process)')\n","parser.add_argument('--pin_memory', default=False, type=bool, help='Use pinned memory or not')\n","parser.add_argument('--prefetch_factor', default=1, type=int, help='Number of sample loaded in advance by each worker')\n","parser.add_argument('--persistent_workers', default=True, type=bool, help='This allows to maintain the workers Dataset instances alive.')\n","parser.add_argument('--tensorboard', default='tsboard_segm', type=str, help='Name for tensorboard run')\n","# args = parser.parse_args()\n","args, unknown = parser.parse_known_args()\n","\n","options = vars(args)  # convert to ordinary dict\n","\n","# lr_steps list or single float\n","lr_steps = ast.literal_eval(options['lr_steps'])\n","if type(lr_steps) is list:\n","    pass\n","elif type(lr_steps) is int:\n","    lr_steps = [lr_steps]\n","else:\n","    raise\n","    \n","options['lr_steps'] = lr_steps\n","\n","logging.info('parsed input parameters:')\n","logging.info(json.dumps(options, indent = 2))\n","main(options)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"marida","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"6e2967a491d864d94a7a9b7b6e79b930ee4f1a6f42bc6fdeff0adc11bbaf92c9"}}},"nbformat":4,"nbformat_minor":2}
